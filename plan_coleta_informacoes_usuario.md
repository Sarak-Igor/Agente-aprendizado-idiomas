# Plano: Sistema de Coleta de Informa√ß√µes do Usu√°rio

## Objetivo
Implementar sistema completo de capta√ß√£o, an√°lise e armazenamento de informa√ß√µes do usu√°rio para:
- Identificar n√≠vel de conhecimento baseado em intera√ß√£o
- Acompanhar evolu√ß√£o ao longo do tempo
- Personalizar intera√ß√µes e auxiliar no desenvolvimento do aprendizado
- Otimizar armazenamento para hist√≥rico de evolu√ß√£o eficiente
- Implementar busca sem√¢ntica para recupera√ß√£o inteligente de contexto

## M√≥dulo de Banco Vetorial

### Estrutura do M√≥dulo
O sistema utilizar√° um **m√≥dulo dedicado** (`backend/app/modules/vector_db/`) para gerenciar todas as opera√ß√µes relacionadas a banco vetorial. Este m√≥dulo ser√°:

- **Modular**: Pode ser usado independentemente ou integrado
- **Extens√≠vel**: Suporta diferentes implementa√ß√µes (ChromaDB, Pinecone, pgvector)
- **Otimizado**: Focado em performance e escalabilidade
- **Isolado**: Separa√ß√£o clara de responsabilidades

### Responsabilidades do M√≥dulo
- Gera√ß√£o e armazenamento de embeddings
- Busca sem√¢ntica e recupera√ß√£o de contexto
- Gerenciamento de collections/namespaces
- Sincroniza√ß√£o com PostgreSQL
- Otimiza√ß√£o e manuten√ß√£o

## Arquitetura Geral

### Sistema de Tradu√ß√£o para Ingl√™s (Normaliza√ß√£o de Dados)

**IMPORTANTE**: Todas as informa√ß√µes coletadas e analisadas devem ser armazenadas em **ingl√™s** para padroniza√ß√£o e melhor performance dos modelos. A exibi√ß√£o e intera√ß√£o com o usu√°rio permanece no idioma selecionado.

#### Regra de Normaliza√ß√£o
- **Armazenamento**: Sempre em ingl√™s (banco de dados estrutural e banco vetorial)
- **Exibi√ß√£o**: No idioma do usu√°rio (tradu√ß√£o reversa quando necess√°rio)
- **An√°lise**: Processamento em ingl√™s para consist√™ncia

#### Implementa√ß√£o

**Servi√ßo de Normaliza√ß√£o**: `backend/app/services/language_normalizer.py`

**Responsabilidades**:
- Traduzir textos para ingl√™s antes de armazenar
- Traduzir de volta para idioma do usu√°rio na exibi√ß√£o
- Normalizar t√≥picos, erros e vocabul√°rio para ingl√™s
- Manter cache de tradu√ß√µes frequentes

**M√©todos principais**:
```python
class LanguageNormalizer:
    def normalize_for_storage(
        self,
        text: str,
        source_language: str,
        target_language: str = "en"
    ) -> str:
        """Traduz texto para ingl√™s antes de armazenar"""
        
    def normalize_for_display(
        self,
        text: str,
        source_language: str = "en",
        target_language: str
    ) -> str:
        """Traduz texto de ingl√™s para idioma do usu√°rio"""
        
    def normalize_topics(
        self,
        topics: List[str],
        source_language: str
    ) -> List[str]:
        """Normaliza lista de t√≥picos para ingl√™s"""
        
    def normalize_error_types(
        self,
        error_data: Dict,
        source_language: str
    ) -> Dict:
        """Normaliza tipos de erro para ingl√™s"""
```

**Pontos de Integra√ß√£o**:
1. **MessageAnalyzer**: Analisa em ingl√™s, armazena em ingl√™s
2. **VectorDB**: Embeddings gerados de textos em ingl√™s
3. **SessionContextManager**: Contexto armazenado em ingl√™s
4. **ProgressTracker**: M√©tricas e evolu√ß√£o em ingl√™s
5. **API Responses**: Traduzir de volta para idioma do usu√°rio

**Estrat√©gia de Tradu√ß√£o** (Baseada no Sistema de V√≠deos):

**Prioridade 1: Ferramentas de Tradu√ß√£o** (deep-translator, googletrans)
- Mais r√°pidas (50-200ms por tradu√ß√£o)
- N√£o consomem tokens de LLM
- Boa qualidade para textos simples e termos t√©cnicos
- Reutiliza infraestrutura existente do sistema de v√≠deos

**Prioridade 2: LLM** (Gemini, etc.)
- Melhor qualidade para contextos complexos
- Fallback quando ferramentas falham
- Usa API keys free dispon√≠veis (j√° configuradas)

**Cache Agressivo**:
- Cache de tradu√ß√µes comuns (ex: "verb_tense" sempre = "verb_tense")
- Dicion√°rio de termos t√©cnicos pr√©-definidos
- Cache em mem√≥ria ou Redis para performance

**Fallback**:
- Se todas as ferramentas falharem, armazenar texto original com flag
- Tentar novamente em processamento ass√≠ncrono
- Notificar se falha persistir

### Fluxo de Coleta de Dados (Atualizado)

```mermaid
flowchart TD
    A[Usu√°rio Envia Mensagem] --> B[ChatService.send_message]
    B --> C[Criar ChatMessage]
    C --> D[LanguageNormalizer.normalize]
    D --> E[Traduzir para Ingl√™s]
    E --> F[MessageAnalyzer.analyze]
    F --> G{An√°lise Ass√≠ncrona}
    G -->|Erros Gramaticais| H[Extrair Erros em Ingl√™s]
    G -->|Vocabul√°rio| I[Extrair Vocabul√°rio em Ingl√™s]
    G -->|Dificuldade| J[Calcular Dificuldade]
    G -->|T√≥picos| K[Identificar T√≥picos em Ingl√™s]
    H --> L[Atualizar ChatMessage]
    I --> L
    J --> L
    K --> L
    L --> M[VectorDB.store_embedding]
    M --> N[Gerar Embedding de Texto em Ingl√™s]
    N --> O[Armazenar no Banco Vetorial]
    O --> P[ContextEnricher.retrieve_context]
    P --> Q[Busca Sem√¢ntica]
    Q --> R[Recuperar Contexto Relevante]
    R --> S[Traduzir Contexto para Idioma do Usu√°rio]
    S --> T[Gerar Resposta LLM]
    T --> U[Atualizar Session Context em Ingl√™s]
    U --> V[Atualizar User Profile em Ingl√™s]
    V --> W[Batch Processor]
    W -->|No Startup| X[An√°lise de Progresso]
    X --> Y[Atualizar Learning Context em Ingl√™s]
    Y --> Z[Calcular N√≠vel]
    Z --> AA[Atualizar Proficiency Level]
```

### Estrutura de Dados

#### 1. An√°lise de Mensagem (ChatMessage)
- `grammar_errors` (JSONB): Lista de erros detectados (em ingl√™s)
- `vocabulary_suggestions` (JSONB): Palavras e sugest√µes (em ingl√™s)
- `difficulty_score` (Float): 0.0 a 1.0
- `topics` (JSONB): T√≥picos identificados (em ingl√™s)
- `analysis_metadata` (JSONB): Metadados da an√°lise
  - `analyzed_at`: Timestamp da an√°lise
  - `analyzer_version`: Vers√£o do analisador usado
  - `confidence_scores`: Scores de confian√ßa por tipo de an√°lise
  - `processing_time_ms`: Tempo de processamento em milissegundos
  - `original_language`: Idioma original da mensagem
  - `normalized_language`: Idioma normalizado (sempre "en")

#### 2. Contexto de Sess√£o (ChatSession.session_context)
**Nota**: Todos os dados armazenados em ingl√™s
```json
{
  "topics_discussed": ["greetings", "food"],
  "common_errors": {
    "verb_tense": {"count": 5, "examples": [...], "last_seen": "2024-02-01T10:30:00Z"},
    "articles": {"count": 3, "examples": [...], "last_seen": "2024-02-01T10:25:00Z"}
  },
  "vocabulary_used": {
    "new_words": ["restaurant", "airport"],
    "total_unique_words": 45,
    "words_by_difficulty": {"easy": 20, "medium": 15, "hard": 10}
  },
  "difficulty_trend": "increasing",
  "session_insights": {
    "most_common_error": "verb_tense",
    "improvement_areas": ["past_tense"],
    "session_start_time": "2024-02-01T10:00:00Z",
    "last_activity": "2024-02-01T10:30:00Z"
  }
}
```

#### 3. Contexto de Aprendizado (UserProfile.learning_context)
```json
{
  "proficiency_evolution": [
    {"date": "2024-01-01", "level": "beginner", "score": 0.3},
    {"date": "2024-02-01", "level": "intermediate", "score": 0.55}
  ],
  "recurring_errors": [
    {"type": "verb_tense", "frequency": 15, "last_seen": "2024-02-01", "severity": "high"}
  ],
  "vocabulary_acquired": {
    "total_words": 250,
    "mastered_words": 180,
    "learning_words": 70
  },
  "topics_mastery": {
    "greetings": 0.9,
    "food": 0.7,
    "travel": 0.5
  },
  "learning_preferences": {
    "style": "conversational",
    "preferred_topics": ["travel"],
    "best_time": "evening"
  }
}
```

#### 4. Banco Vetorial (Vector Database)
**Collections/Namespaces**:

**IMPORTANTE**: Todas as collections s√£o separadas por `user_id` para isolamento de dados. Filtros por `user_id` s√£o obrigat√≥rios em todas as opera√ß√µes.

- **user_messages**: Embeddings de mensagens do usu√°rio (texto em ingl√™s)
  - Metadata: `user_id` (obrigat√≥rio), `session_id`, `message_id`, `created_at`, `topics` (em ingl√™s), `difficulty_score`, `original_language`
  
- **topics**: Embeddings de t√≥picos discutidos (em ingl√™s)
  - Metadata: `user_id` (obrigat√≥rio), `topic_name` (em ingl√™s), `frequency`, `last_discussed`
  
- **corrections**: Embeddings de erros e corre√ß√µes (em ingl√™s)
  - Metadata: `user_id` (obrigat√≥rio), `error_type` (em ingl√™s), `original_text` (em ingl√™s), `corrected_text` (em ingl√™s), `frequency`
  
- **vocabulary**: Embeddings de vocabul√°rio aprendido (em ingl√™s)
  - Metadata: `user_id` (obrigat√≥rio), `word` (em ingl√™s), `context` (em ingl√™s), `mastery_level`, `first_seen`, `last_used`
  
- **session_contexts**: Embeddings de contextos de sess√µes (em ingl√™s)
  - Metadata: `user_id` (obrigat√≥rio), `session_id`, `summary` (em ingl√™s), `key_insights` (em ingl√™s)

**Isolamento por Usu√°rio**:
- Todas as buscas devem incluir filtro `user_id` obrigat√≥rio
- Collections podem ser particionadas por usu√°rio (namespace por user_id)
- Backup e limpeza podem ser feitos por usu√°rio espec√≠fico
- N√£o h√° necessidade de criptografia, apenas isolamento l√≥gico

## Vis√£o Geral das Fases

O plano est√° dividido em **7 fases progressivas**:

- **Fases 1-4**: Sistema b√°sico de coleta e an√°lise (sem banco vetorial)
  - ‚úÖ Fase 1: An√°lise b√°sica de mensagens - **CONCLU√çDA**
  - ‚è∏Ô∏è Fase 2: Atualiza√ß√£o de contexto de sess√£o - **PAUSADA**
  - ‚è∏Ô∏è Fase 3: Tracking de progresso e evolu√ß√£o - **PENDENTE**
  - ‚è∏Ô∏è Fase 4: Personaliza√ß√£o e utiliza√ß√£o do contexto - **PENDENTE**

- **Fases 5-7**: Implementa√ß√£o de banco vetorial e busca sem√¢ntica
  - ‚è∏Ô∏è Fase 5: Infraestrutura de banco vetorial - **PENDENTE**
  - ‚è∏Ô∏è Fase 6: Busca sem√¢ntica e RAG - **PENDENTE**
  - ‚è∏Ô∏è Fase 7: Integra√ß√£o completa e otimiza√ß√£o - **PENDENTE**

**Nota**: As Fases 1-4 podem ser implementadas independentemente. As Fases 5-7 dependem da infraestrutura das fases anteriores, mas podem ser implementadas em paralelo ap√≥s a Fase 2.

**Status Atual do Projeto**:
- ‚úÖ **Fase 1 conclu√≠da**: Sistema de an√°lise b√°sica implementado e funcionando
- ‚è∏Ô∏è **Pausado para melhorias no chat**: Priorizando melhorias na interface de sele√ß√£o de modelos e qualidade da conversa antes de continuar com a coleta de dados
- üìù **Decis√£o estrat√©gica**: Melhorar UX do chat primeiro para garantir que os dados coletados venham de conversas de alta qualidade

## Fase 1: An√°lise B√°sica de Mensagens ‚úÖ CONCLU√çDA

### Objetivo
Implementar an√°lise de mensagens do usu√°rio para extrair erros, vocabul√°rio e dificuldade. **An√°lise ser√° processada de forma ass√≠ncrona** para n√£o impactar lat√™ncia da resposta.

### Status da Implementa√ß√£o
**Data de Conclus√£o**: 2025-01-19

**Tarefas Conclu√≠das**:
- ‚úÖ 1.0 LanguageNormalizer Service criado e funcionando
- ‚úÖ 1.1 MessageAnalyzer Service criado e funcionando
- ‚úÖ 1.2 Integra√ß√£o no ChatService com processamento ass√≠ncrono
- ‚úÖ 1.3 Campos `topics` e `analysis_metadata` adicionados ao modelo
- ‚úÖ 1.4 Schemas Pydantic criados e ChatMessageResponse atualizado
- ‚úÖ 1.5 Valida√ß√£o de dados implementada
- ‚úÖ Migra√ß√£o de banco de dados executada com sucesso
- ‚úÖ Script de verifica√ß√£o criado (`backend/verify_context_saving.py`)

**Arquivos Criados**:
- `backend/app/services/language_normalizer.py` - Normaliza√ß√£o de idioma
- `backend/app/services/message_analyzer.py` - An√°lise de mensagens
- `backend/app/schemas/analysis_schemas.py` - Schemas de valida√ß√£o
- `backend/migrate_add_analysis_fields.py` - Script de migra√ß√£o
- `backend/verify_context_saving.py` - Script de verifica√ß√£o de contexto
- `docs/erros_gemini_api.md` - Documenta√ß√£o de erros da API Gemini

**Arquivos Modificados**:
- `backend/app/models/database.py` - Adicionados campos `topics` e `analysis_metadata`
- `backend/app/services/chat_service.py` - Integra√ß√£o de an√°lise ass√≠ncrona
- `backend/app/schemas/schemas.py` - Atualizado `ChatMessageResponse`
- `backend/app/api/routes/chat.py` - Atualizado para retornar novos campos
- `backend/app/services/model_router.py` - Melhorias no roteamento de modelos
- `backend/app/services/llm_service.py` - Melhorias no tratamento de erros e roteamento
- `backend/app/services/gemini_service.py` - Melhorias no tratamento de erros
- `backend/app/services/token_usage_service.py` - user_id opcional

**Notas de Implementa√ß√£o**:
- Processamento ass√≠ncrono implementado usando threading (sem depend√™ncia de fila externa)
- Cache de tradu√ß√µes em mem√≥ria com TTL de 30 dias
- Valida√ß√£o Pydantic antes de armazenar no banco
- Migra√ß√£o de banco executada: colunas `topics` e `analysis_metadata` adicionadas sem perda de dados
- Sistema de fallback de tradu√ß√£o: deep-translator ‚Üí googletrans ‚Üí gemini
- **Melhorias no roteamento de modelos**:
  - Bloqueio tempor√°rio de modelos (n√£o permanente) com TTL configur√°vel
  - Listagem din√¢mica de modelos quando todos est√£o bloqueados
  - Filtro de modelos inadequados (embedding, veo, etc.) para gera√ß√£o de texto
  - Aumento de tentativas baseado em modelos dispon√≠veis (at√© 50 tentativas)
  - Tratamento melhorado de erros 404 e 429
  - Limpeza autom√°tica de bloqueios tempor√°rios expirados

**Testes Realizados**:
- ‚úÖ Verifica√ß√£o de salvamento de contexto executada com sucesso
- ‚úÖ An√°lise ass√≠ncrona funcionando (metadata e difficulty_score sendo salvos)
- ‚úÖ Contexto da conversa sendo constru√≠do corretamente
- ‚úÖ Mensagens sendo salvas no banco de dados

**Status Atual**:
- ‚úÖ Fase 1 implementada e funcional
- ‚è∏Ô∏è **PAUSADO**: Decis√£o de melhorar chat (sele√ß√£o de modelos e qualidade da conversa) antes de continuar com Fase 2
- üìù **Nota**: Dados coletados ser√£o zerados posteriormente (em desenvolvimento)

**Pr√≥ximos Passos**:
- Melhorar interface de sele√ß√£o de modelos no chat
- Melhorar qualidade da conversa (prompts e contexto)
- Retomar Fase 2 ap√≥s melhorias no chat

### Tarefas

#### 1.0 Criar LanguageNormalizer Service
**Arquivo**: `backend/app/services/language_normalizer.py`

**Responsabilidades**:
- Traduzir textos para ingl√™s antes de armazenar
- Traduzir de volta para idioma do usu√°rio na exibi√ß√£o
- Normalizar t√≥picos, erros e vocabul√°rio
- Cache de tradu√ß√µes frequentes
- **Usar ferramentas de tradu√ß√£o (prioridade) ou LLM (fallback)**

**Estrat√©gia de Tradu√ß√£o**:
1. **Prioridade 1**: Ferramentas de tradu√ß√£o (deep-translator, googletrans)
   - Mais r√°pidas (50-200ms)
   - N√£o consomem tokens de LLM
   - Boa qualidade para textos simples
2. **Prioridade 2**: LLM (Gemini, etc.)
   - Melhor qualidade para contextos complexos
   - Fallback quando ferramentas falham
   - Usa API keys free dispon√≠veis

**M√©todos principais**:
```python
class LanguageNormalizer:
    def __init__(self, translation_service_factory: TranslationServiceFactory):
        # Usa factory existente do sistema de v√≠deos
        self.translation_service = translation_service_factory.create_auto_fallback(
            preferred_service="deeptranslator",  # Mais r√°pido
            fallback_services=["googletrans", "gemini"],  # Fallbacks
            configs={...}
        )
        self.cache = {}  # Cache em mem√≥ria (pode usar Redis)
        
    def normalize_for_storage(
        self,
        text: str,
        source_language: str
    ) -> str:
        """Traduz texto para ingl√™s antes de armazenar"""
        # Verifica cache primeiro
        cached = self._get_cached_translation(text, source_language, "en")
        if cached:
            return cached
            
        # Tenta ferramenta de tradu√ß√£o primeiro (r√°pido)
        try:
            translated = self.translation_service.translate_text(
                text, "en", source_language
            )
            self._cache_translation(text, source_language, "en", translated)
            return translated
        except Exception as e:
            logger.warning(f"Falha na tradu√ß√£o com ferramenta: {e}")
            # Fallback para LLM se necess√°rio
            raise
        
    def normalize_for_display(
        self,
        text: str,
        target_language: str
    ) -> str:
        """Traduz texto de ingl√™s para idioma do usu√°rio"""
        if target_language == "en":
            return text  # J√° est√° em ingl√™s
            
        cached = self._get_cached_translation(text, "en", target_language)
        if cached:
            return cached
            
        translated = self.translation_service.translate_text(
            text, target_language, "en"
        )
        self._cache_translation(text, "en", target_language, translated)
        return translated
        
    def normalize_topics(self, topics: List[str], source_language: str) -> List[str]
    def normalize_error_types(self, error_data: Dict, source_language: str) -> Dict
    def _get_cached_translation(self, text: str, source: str, target: str) -> Optional[str]
    def _cache_translation(self, text: str, source: str, target: str, translated: str)
```

**Compara√ß√£o de Performance**:

| M√©todo | Velocidade | Qualidade | Custo | Uso Recomendado |
|--------|-----------|----------|-------|----------------|
| deep-translator | 50-200ms | Boa | Gr√°tis | Textos simples, termos t√©cnicos |
| googletrans | 100-300ms | Boa | Gr√°tis | Fallback de deep-translator |
| LLM (Gemini) | 1-3s | Excelente | Free tier | Contextos complexos, fallback |

#### 1.1 Criar MessageAnalyzer Service
**Arquivo**: `backend/app/services/message_analyzer.py`

**Responsabilidades**:
- Analisar mensagens do usu√°rio usando LLM (mensagem j√° normalizada para ingl√™s)
- Extrair erros gramaticais estruturados (em ingl√™s)
- Identificar vocabul√°rio usado (em ingl√™s)
- Calcular dificuldade da mensagem
- Identificar t√≥picos principais (em ingl√™s)
- **Processamento ass√≠ncrono** para n√£o bloquear resposta

**M√©todos principais**:
```python
class MessageAnalyzer:
    def analyze_message(
        self, 
        message: str,  # J√° normalizada para ingl√™s
        language: str,  # Idioma original
        user_level: str
    ) -> Dict:
        """Analisa mensagem e retorna an√°lise estruturada (tudo em ingl√™s)"""
        
    def analyze_message_async(
        self,
        message_id: UUID,
        message: str,
        language: str,
        user_level: str
    ) -> None:
        """Analisa mensagem de forma ass√≠ncrona (background job)"""
        
    def _extract_grammar_errors(self, message: str, language: str) -> List[Dict]
    def _extract_vocabulary(self, message: str, language: str) -> Dict
    def _calculate_difficulty(self, message: str, language: str, user_level: str) -> float
    def _identify_topics(self, message: str) -> List[str]
    def _build_analysis_metadata(self, processing_time: float) -> Dict
```

#### 1.2 Integrar An√°lise no ChatService
**Arquivo**: `backend/app/services/chat_service.py`

**Modifica√ß√µes**:
- Adicionar `MessageAnalyzer` e `LanguageNormalizer` no `__init__`
- **Fluxo s√≠ncrono**: Criar `user_message`, retornar resposta imediata
- **Fluxo ass√≠ncrono**: Enfileirar an√°lise em background ap√≥s resposta
- Armazenar resultados em `grammar_errors`, `vocabulary_suggestions`, `difficulty_score`, `topics`
- Adicionar campo `topics` e `analysis_metadata` em `ChatMessage` (migration)

**Fluxo de Processamento**:
1. Usu√°rio envia mensagem
2. Normalizar mensagem para ingl√™s
3. Criar ChatMessage (sem an√°lise ainda)
4. Gerar resposta imediata (sem an√°lise)
5. Enfileirar an√°lise ass√≠ncrona
6. Background job: Analisar, atualizar ChatMessage, atualizar contexto

#### 1.3 Criar Migration para Campos Adicionais
**Arquivo**: `backend/migrations/add_analysis_fields_to_chat_message.py`

Adicionar colunas:
- `topics` (JSONB): T√≥picos identificados (em ingl√™s)
- `analysis_metadata` (JSONB): Metadados da an√°lise

#### 1.4 Atualizar Schemas
**Arquivo**: `backend/app/schemas/schemas.py`

Adicionar:
- `topics` em `ChatMessageResponse`
- `analysis_metadata` em `ChatMessageResponse`
- Schemas Pydantic para valida√ß√£o de estruturas JSONB (ver Fase 1.5)

#### 1.5 Implementar Valida√ß√£o de Dados
**Arquivo**: `backend/app/schemas/analysis_schemas.py`

Criar schemas Pydantic para valida√ß√£o de estruturas JSONB:
```python
class GrammarErrorSchema(BaseModel):
    type: str  # Em ingl√™s
    original: str
    corrected: str
    explanation: Optional[str] = None
    confidence: float  # 0.0 a 1.0

class VocabularySuggestionSchema(BaseModel):
    word: str  # Em ingl√™s
    suggestion: Optional[str] = None
    context: Optional[str] = None
    difficulty: str  # "easy", "medium", "hard"

class AnalysisMetadataSchema(BaseModel):
    analyzed_at: datetime
    analyzer_version: str
    confidence_scores: Dict[str, float]
    processing_time_ms: float
    original_language: str
    normalized_language: str = "en"
```

**Uso**: Validar dados antes de armazenar em JSONB

## Fase 2: Atualiza√ß√£o de Contexto de Sess√£o ‚è∏Ô∏è PAUSADA

### Objetivo
Implementar atualiza√ß√£o din√¢mica do `session_context` durante a sess√£o.

### Status
‚è∏Ô∏è **PAUSADA** - Priorizando melhorias no chat (sele√ß√£o de modelos e qualidade da conversa) antes de continuar com a coleta de dados.

### Tarefas

#### 2.1 Criar SessionContextManager
**Arquivo**: `backend/app/services/session_context_manager.py`

**Responsabilidades**:
- Gerenciar contexto da sess√£o
- Agregar erros, vocabul√°rio e t√≥picos
- Calcular tend√™ncias (dificuldade, progresso)
- Identificar √°reas de melhoria

**M√©todos principais**:
```python
class SessionContextManager:
    def update_session_context(
        self,
        session: ChatSession,
        message_analysis: Dict
    ) -> Dict:
        """Atualiza contexto da sess√£o com nova an√°lise"""
        
    def _aggregate_errors(self, session_context: Dict, new_errors: List) -> Dict
    def _aggregate_vocabulary(self, session_context: Dict, new_vocab: Dict) -> Dict
    def _update_topics(self, session_context: Dict, new_topics: List) -> List
    def _calculate_trends(self, session_context: Dict) -> Dict
```

#### 2.2 Integrar no ChatService
**Arquivo**: `backend/app/services/chat_service.py`

**Modifica√ß√µes**:
- Adicionar `SessionContextManager` no `__init__`
- Chamar `update_session_context` ap√≥s an√°lise de mensagem
- Salvar `session_context` atualizado no banco

#### 2.3 Inicializar Session Context
**Arquivo**: `backend/app/services/chat_service.py`

Modificar `create_session` para inicializar `session_context` vazio.

## Fase 3: Tracking de Progresso e Evolu√ß√£o

### Objetivo
Implementar sistema de acompanhamento de evolu√ß√£o do usu√°rio ao longo do tempo.

### Tarefas

#### 3.1 Criar ProgressTracker Service
**Arquivo**: `backend/app/services/progress_tracker.py`

**Responsabilidades**:
- Calcular progresso baseado em m√©tricas
- Atualizar `learning_context` do `UserProfile`
- Identificar erros recorrentes
- Calcular evolu√ß√£o de vocabul√°rio
- Determinar mudan√ßas de n√≠vel

**M√©todos principais**:
```python
class ProgressTracker:
    def update_user_progress(
        self,
        user_id: UUID,
        session_analysis: Dict
    ) -> Dict:
        """Atualiza progresso do usu√°rio"""
        
    def _calculate_proficiency_score(self, user_profile: UserProfile) -> float
    def _identify_recurring_errors(self, user_id: UUID) -> List[Dict]
    def _update_vocabulary_stats(self, user_id: UUID, new_words: List) -> Dict
    def _check_level_change(self, user_profile: UserProfile) -> Optional[str]
```

#### 3.2 Criar Batch Processor
**Arquivo**: `backend/app/services/batch_analyzer.py`

**Responsabilidades**:
- Processar an√°lises em lote (ass√≠ncrono)
- Analisar m√∫ltiplas sess√µes
- Calcular m√©tricas agregadas
- Atualizar `learning_context` periodicamente

**M√©todos principais**:
```python
class BatchAnalyzer:
    def process_all_pending_messages(
        self,
        batch_size: int = 100
    ) -> Dict:
        """Processa todas as mensagens pendentes (n√£o analisadas)"""
        # Busca mensagens do usu√°rio sem an√°lise completa
        # Agrupa em lotes de batch_size
        # Processa cada lote
        
    def process_user_sessions(
        self,
        user_id: UUID,
        days: int = 30,
        batch_size: int = 100
    ) -> Dict:
        """Processa sess√µes do usu√°rio e atualiza progresso"""
        
    def process_user_pending_messages(
        self,
        user_id: UUID,
        batch_size: int = 100
    ) -> Dict:
        """Processa mensagens pendentes de um usu√°rio espec√≠fico"""
        # Busca mensagens do usu√°rio (role="user") sem analysis_metadata
        # ou com analysis_metadata indicando an√°lise incompleta
        
    def _get_pending_messages(
        self,
        limit: int = 100
    ) -> List[ChatMessage]:
        """Retorna mensagens pendentes de an√°lise"""
        # Filtra: role="user" AND (analysis_metadata IS NULL OR incomplete)
        
    def _analyze_error_patterns(self, messages: List[ChatMessage]) -> Dict
    def _analyze_vocabulary_evolution(self, messages: List[ChatMessage]) -> Dict
    def _calculate_improvement_metrics(self, user_id: UUID) -> Dict
    def _handle_batch_failure(self, user_id: UUID, error: Exception) -> None
```

**Nota sobre Mensagens**:
- **Mensagens do usu√°rio**: `ChatMessage.role == "user"` (mensagens enviadas pelo usu√°rio)
- **Mensagens pendentes**: Mensagens sem `analysis_metadata` ou com an√°lise incompleta
- **Batch size**: Refere-se a quantidade de mensagens do usu√°rio processadas por vez

#### 3.3 Integrar Batch Processing no Startup
**Arquivo**: `backend/app/main.py`

**Modifica√ß√µes**:
- Adicionar evento `@app.on_event("startup")` para executar batch processing
- Executar em background thread para n√£o bloquear startup
- Processar apenas mensagens pendentes (n√£o analisadas)

**Implementa√ß√£o**:
```python
from fastapi import FastAPI
import threading
from app.services.batch_analyzer import BatchAnalyzer
from app.database import SessionLocal

@app.on_event("startup")
async def startup_batch_processing():
    """Executa batch processing quando backend inicia"""
    def run_batch():
        db = SessionLocal()
        try:
            batch_analyzer = BatchAnalyzer(db)
            # Processa apenas mensagens pendentes
            result = batch_analyzer.process_all_pending_messages()
            logger.info(f"Batch processing conclu√≠do: {result}")
        except Exception as e:
            logger.error(f"Erro no batch processing: {e}")
        finally:
            db.close()
    
    # Executa em thread separada para n√£o bloquear startup
    thread = threading.Thread(target=run_batch, daemon=True)
    thread.start()
    logger.info("Batch processing iniciado em background")
```

**Alternativa (se quiser executar ap√≥s startup completo)**:
```python
@app.on_event("startup")
async def startup_batch_processing():
    """Executa batch processing ap√≥s startup completo"""
    # Aguarda alguns segundos para garantir que API est√° pronta
    import asyncio
    await asyncio.sleep(5)
    
    def run_batch():
        # ... mesmo c√≥digo acima
        pass
    
    thread = threading.Thread(target=run_batch, daemon=True)
    thread.start()
```

#### 3.4 Integrar no ChatService
**Arquivo**: `backend/app/services/chat_service.py`

Chamar `ProgressTracker` ap√≥s an√°lise de mensagem (opcionalmente ass√≠ncrono).

## Fase 4: Personaliza√ß√£o e Utiliza√ß√£o do Contexto

### Objetivo
Utilizar informa√ß√µes coletadas para personalizar intera√ß√µes e melhorar aprendizado.

### Tarefas

#### 4.1 Criar ContextEnricher Service
**Arquivo**: `backend/app/services/context_enricher.py`

**Responsabilidades**:
- Enriquecer prompts com contexto do usu√°rio
- Recuperar erros recorrentes relevantes
- Incluir t√≥picos de interesse
- Ajustar dificuldade baseado em progresso

**M√©todos principais**:
```python
class ContextEnricher:
    def enrich_system_prompt(
        self,
        base_prompt: str,
        session: ChatSession,
        user_profile: UserProfile
    ) -> str:
        """Enriquece prompt com contexto do usu√°rio"""
        
    def _get_relevant_errors(self, session: ChatSession) -> List[Dict]
    def _get_user_preferences(self, user_profile: UserProfile) -> Dict
    def _adjust_difficulty(self, user_profile: UserProfile) -> str
```

#### 4.2 Atualizar System Prompt Builder
**Arquivo**: `backend/app/services/chat_service.py`

Modificar `_build_system_prompt` para usar `ContextEnricher` e incluir:
- Erros recorrentes do usu√°rio
- T√≥picos de interesse
- Progresso recente
- √Åreas de melhoria

#### 4.3 Criar Endpoint de Insights
**Arquivo**: `backend/app/api/routes/chat.py`

Endpoint `GET /api/chat/insights/{user_id}` para retornar:
- Progresso do usu√°rio
- Erros recorrentes
- Vocabul√°rio aprendido
- Recomenda√ß√µes de estudo

#### 4.4 Atualizar Frontend (Opcional)
**Arquivo**: `frontend/src/components/Chat/Chat.tsx`

Adicionar visualiza√ß√£o de:
- Progresso atual
- Erros comuns
- Vocabul√°rio aprendido

## Fase 5: Implementa√ß√£o de Banco Vetorial - Infraestrutura

### Objetivo
Implementar infraestrutura de banco vetorial para armazenar e buscar informa√ß√µes do usu√°rio de forma sem√¢ntica.

### Tarefas

#### 5.1 Criar M√≥dulo de Banco Vetorial
**Arquivo**: `backend/app/modules/vector_db/__init__.py`

Criar m√≥dulo dedicado para gerenciamento de banco vetorial com estrutura modular:
```
backend/app/modules/vector_db/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ vector_store.py          # Interface e implementa√ß√£o base
‚îú‚îÄ‚îÄ embedding_service.py     # Gera√ß√£o de embeddings
‚îú‚îÄ‚îÄ collections.py           # Defini√ß√£o de collections/namespaces
‚îî‚îÄ‚îÄ config.py               # Configura√ß√µes do banco vetorial
```

#### 5.2 Criar VectorStore Interface
**Arquivo**: `backend/app/modules/vector_db/vector_store.py`

**Responsabilidades**:
- Interface abstrata para diferentes implementa√ß√µes (ChromaDB, Pinecone, pgvector)
- Opera√ß√µes CRUD de embeddings
- Busca por similaridade
- Gerenciamento de collections

**M√©todos principais**:
```python
class VectorStore:
    def store_embedding(
        self,
        collection: str,
        text: str,
        embedding: List[float],
        metadata: Dict
    ) -> str:
        """Armazena embedding com metadados"""
        
    def search_similar(
        self,
        collection: str,
        query_embedding: List[float],
        limit: int = 10,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """Busca itens similares"""
        
    def delete_by_id(self, collection: str, id: str) -> bool
    def update_metadata(self, collection: str, id: str, metadata: Dict) -> bool
```

#### 5.3 Criar EmbeddingService
**Arquivo**: `backend/app/modules/vector_db/embedding_service.py`

**Responsabilidades**:
- Gerar embeddings usando modelos de embedding
- Suportar m√∫ltiplos modelos (OpenAI, Sentence-BERT, etc.)
- Cache de embeddings para otimiza√ß√£o
- Normaliza√ß√£o de embeddings

**M√©todos principais**:
```python
class EmbeddingService:
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2")
    
    def generate_embedding(self, text: str) -> List[float]:
        """Gera embedding para texto"""
        
    def generate_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Gera embeddings em lote"""
        
    def _load_model(self)
    def _normalize_embedding(self, embedding: List[float]) -> List[float]
```

#### 5.4 Implementar ChromaDB Store (ou alternativa)
**Arquivo**: `backend/app/modules/vector_db/chroma_store.py`

Implementa√ß√£o concreta usando ChromaDB (ou Pinecone/pgvector):
- Inicializa√ß√£o e conex√£o
- Cria√ß√£o de collections
- Opera√ß√µes de armazenamento e busca
- Gerenciamento de persist√™ncia

#### 5.5 Definir Collections
**Arquivo**: `backend/app/modules/vector_db/collections.py`

Definir collections/namespaces para diferentes tipos de dados:
- `user_messages`: Mensagens do usu√°rio
- `topics`: T√≥picos discutidos
- `corrections`: Erros e corre√ß√µes
- `vocabulary`: Vocabul√°rio aprendido
- `session_contexts`: Contextos de sess√µes

#### 5.6 Configurar Banco Vetorial
**Arquivo**: `backend/app/modules/vector_db/config.py`

Configura√ß√µes:
- Tipo de banco vetorial (ChromaDB, Pinecone, pgvector)
- Caminho de persist√™ncia
- Modelo de embedding
- Dimens√µes de embedding
- Configura√ß√µes de conex√£o

## Fase 6: Busca Sem√¢ntica e RAG

### Objetivo
Implementar busca sem√¢ntica e RAG (Retrieval-Augmented Generation) para recuperar contexto relevante do hist√≥rico do usu√°rio.

### Tarefas

#### 6.1 Criar SemanticSearch Service
**Arquivo**: `backend/app/modules/vector_db/semantic_search.py`

**Responsabilidades**:
- Buscar mensagens similares no hist√≥rico
- Buscar t√≥picos relacionados
- Buscar erros e corre√ß√µes similares
- Buscar vocabul√°rio relacionado

**M√©todos principais**:
```python
class SemanticSearch:
    def search_similar_messages(
        self,
        user_id: UUID,
        query: str,
        limit: int = 5
    ) -> List[Dict]:
        """Busca mensagens similares do usu√°rio"""
        
    def search_related_topics(
        self,
        user_id: UUID,
        topic: str,
        limit: int = 5
    ) -> List[Dict]:
        """Busca t√≥picos relacionados"""
        
    def search_similar_errors(
        self,
        user_id: UUID,
        error_type: str,
        limit: int = 5
    ) -> List[Dict]:
        """Busca erros similares"""
        
    def search_vocabulary_context(
        self,
        user_id: UUID,
        word: str,
        limit: int = 5
    ) -> List[Dict]:
        """Busca contexto de uso de vocabul√°rio"""
```

#### 6.2 Criar RAG Service
**Arquivo**: `backend/app/modules/vector_db/rag_service.py`

**Responsabilidades**:
- Recuperar contexto relevante para prompts
- Combinar m√∫ltiplas buscas sem√¢nticas
- Filtrar e rankear resultados
- Formatar contexto para LLM

**M√©todos principais**:
```python
class RAGService:
    def retrieve_relevant_context(
        self,
        user_id: UUID,
        session: ChatSession,
        query: str,
        context_types: List[str] = ["messages", "topics", "errors"]
    ) -> Dict:
        """Recupera contexto relevante para enriquecer prompt"""
        
    def _combine_search_results(self, results: List[List[Dict]]) -> List[Dict]
    def _rank_by_relevance(self, results: List[Dict], query: str) -> List[Dict]
    def _format_for_prompt(self, context: Dict) -> str
```

#### 6.3 Integrar Armazenamento de Embeddings
**Arquivo**: `backend/app/services/chat_service.py`

**Modifica√ß√µes**:
- Ap√≥s an√°lise de mensagem, gerar embedding
- Armazenar embedding no banco vetorial com metadados
- Armazenar embeddings de t√≥picos, erros e vocabul√°rio

#### 6.4 Integrar Busca Sem√¢ntica no ContextEnricher
**Arquivo**: `backend/app/services/context_enricher.py`

**Modifica√ß√µes**:
- Usar `RAGService` para recuperar contexto relevante
- Incluir mensagens similares no prompt
- Incluir erros e corre√ß√µes relacionadas
- Incluir t√≥picos e vocabul√°rio relacionados

## Fase 7: Integra√ß√£o Completa e Otimiza√ß√£o

### Objetivo
Integrar banco vetorial completamente no sistema e otimizar para uso em produ√ß√£o.

### Tarefas

#### 7.1 Criar Batch Embedding Processor
**Arquivo**: `backend/app/modules/vector_db/batch_embedding_processor.py`

**Responsabilidades**:
- Processar mensagens antigas em lote para gerar embeddings
- Migrar dados existentes para banco vetorial
- Atualizar embeddings quando necess√°rio
- Limpar embeddings obsoletos

**M√©todos principais**:
```python
class BatchEmbeddingProcessor:
    def process_historical_messages(
        self,
        user_id: UUID,
        days: int = 90
    ) -> Dict:
        """Processa mensagens hist√≥ricas para gerar embeddings"""
        
    def migrate_existing_data(self, user_id: UUID) -> Dict
    def update_embeddings(self, message_ids: List[UUID]) -> Dict
    def cleanup_old_embeddings(self, days: int = 365) -> int
```

#### 7.2 Criar VectorDB Manager
**Arquivo**: `backend/app/modules/vector_db/manager.py`

**Responsabilidades**:
- Gerenciar ciclo de vida do banco vetorial
- Sincroniza√ß√£o com PostgreSQL
- Backup e restore
- Monitoramento e m√©tricas

**M√©todos principais**:
```python
class VectorDBManager:
    def sync_with_postgres(self, user_id: UUID) -> Dict
    def backup_collection(self, collection: str) -> str
    def restore_collection(self, collection: str, backup_path: str) -> bool
    def get_stats(self, collection: str) -> Dict
```

#### 7.3 Otimizar Busca Sem√¢ntica
**Arquivo**: `backend/app/modules/vector_db/semantic_search.py`

**Otimiza√ß√µes**:
- Cache de buscas frequentes
- √çndices otimizados
- Filtros eficientes por usu√°rio
- Limite de resultados inteligente

#### 7.4 Criar Endpoints de Busca
**Arquivo**: `backend/app/api/routes/chat.py`

Endpoints:
- `GET /api/chat/search/similar-messages` - Buscar mensagens similares
- `GET /api/chat/search/related-topics` - Buscar t√≥picos relacionados
- `GET /api/chat/search/similar-errors` - Buscar erros similares

#### 7.5 Integrar no Fluxo Completo
**Arquivo**: `backend/app/services/chat_service.py`

**Modifica√ß√µes**:
- Armazenar embeddings automaticamente ap√≥s an√°lise
- Usar busca sem√¢ntica para enriquecer contexto
- Integrar RAG no processo de gera√ß√£o de resposta

#### 7.6 Integrar Tarefas de Manuten√ß√£o no Startup
**Arquivo**: `backend/app/main.py`

**Modifica√ß√µes**:
- Adicionar tarefas de manuten√ß√£o no evento `startup`
- Executar em background thread

**Tarefas executadas no startup**:
- Processar embeddings pendentes
- Limpar embeddings antigos (opcional, pode ser manual)
- Sincronizar com PostgreSQL (verificar consist√™ncia)

**Nota**: Como sistema n√£o fica ligado full time, manuten√ß√£o executa no startup. Para limpeza de embeddings antigos, pode ser executada manualmente quando necess√°rio.

## Valida√ß√£o de Dados (Ponto 5 - Detalhado)

### O que √© Pydantic?

**Pydantic** √© uma biblioteca Python que valida dados usando type hints (anota√ß√µes de tipo). √â como um "validador inteligente" que garante que os dados est√£o no formato correto antes de serem usados.

#### Exemplo Simples:

```python
from pydantic import BaseModel

class User(BaseModel):
    name: str  # Deve ser string
    age: int   # Deve ser n√∫mero inteiro
    email: str # Deve ser string

# ‚úÖ V√°lido
user = User(name="Jo√£o", age=25, email="joao@email.com")

# ‚ùå Inv√°lido - age n√£o √© inteiro
user = User(name="Jo√£o", age="vinte e cinco", email="joao@email.com")
# Erro: ValidationError - age deve ser int, n√£o str
```

#### Por que usar Pydantic?

1. **Valida√ß√£o Autom√°tica**: Verifica tipos, formatos, valores m√≠nimos/m√°ximos
2. **Documenta√ß√£o Autom√°tica**: Schemas servem como documenta√ß√£o
3. **Erros Claros**: Mensagens de erro descritivas
4. **Type Safety**: Ajuda a evitar bugs de tipo
5. **Integra√ß√£o**: Funciona bem com FastAPI, SQLAlchemy

### Objetivo
Garantir integridade e consist√™ncia dos dados armazenados em campos JSONB atrav√©s de valida√ß√£o rigorosa usando Pydantic.

### Estrat√©gia de Valida√ß√£o

#### 1. Schemas Pydantic para Estruturas JSONB

Criar schemas espec√≠ficos para cada tipo de dado JSONB:

**Arquivo**: `backend/app/schemas/analysis_schemas.py`

```python
# Valida√ß√£o de erros gramaticais
class GrammarErrorSchema(BaseModel):
    type: str  # Em ingl√™s: "verb_tense", "article", etc.
    original: str
    corrected: str
    explanation: Optional[str] = None
    confidence: float = Field(ge=0.0, le=1.0)
    position: Optional[Dict[str, int]] = None  # {start: 10, end: 15}

# Valida√ß√£o de sugest√µes de vocabul√°rio
class VocabularySuggestionSchema(BaseModel):
    word: str  # Em ingl√™s
    suggestion: Optional[str] = None
    context: Optional[str] = None
    difficulty: Literal["easy", "medium", "hard"]
    frequency: Optional[int] = None

# Valida√ß√£o de t√≥picos
class TopicSchema(BaseModel):
    name: str  # Em ingl√™s
    confidence: float = Field(ge=0.0, le=1.0)
    category: Optional[str] = None

# Valida√ß√£o de metadados de an√°lise
class AnalysisMetadataSchema(BaseModel):
    analyzed_at: datetime
    analyzer_version: str
    confidence_scores: Dict[str, float]
    processing_time_ms: float = Field(ge=0.0)
    original_language: str
    normalized_language: Literal["en"] = "en"
    model_used: Optional[str] = None
```

#### 2. Valida√ß√£o em Camadas

**Camada 1: Valida√ß√£o no Service**
- Validar antes de salvar no banco
- Usar schemas Pydantic para valida√ß√£o
- Retornar erros descritivos

**Camada 2: Valida√ß√£o no Model**
- Validar no momento de cria√ß√£o/atualiza√ß√£o
- Usar validators do SQLAlchemy quando poss√≠vel

**Camada 3: Valida√ß√£o na API**
- Validar dados recebidos na API
- Retornar erros HTTP apropriados

#### 3. Versionamento de Schemas

**O que √© Versionamento de Schemas?**

Quando voc√™ muda a estrutura dos dados (adiciona campos, remove campos, muda tipos), precisa garantir que:
1. Dados antigos ainda funcionem
2. Dados novos usem estrutura atualizada
3. Migra√ß√£o seja suave sem perder dados

**Estrat√©gia de Versionamento**:

**1. Vers√£o no Metadata**:
- Cada an√°lise armazena vers√£o do schema usado
- Permite identificar estrutura dos dados antigos

**2. Suporte a M√∫ltiplas Vers√µes**:
- Sistema suporta ler dados de vers√µes antigas
- Migra√ß√£o autom√°tica quando poss√≠vel

**3. Migra√ß√£o Gradual**:
- Novos dados usam vers√£o atual
- Dados antigos s√£o migrados quando acessados
- N√£o precisa migrar tudo de uma vez

**Exemplo Pr√°tico**:

```python
# Vers√£o 1.0.0 (inicial)
class GrammarErrorSchemaV1(BaseModel):
    type: str
    original: str
    corrected: str

# Vers√£o 2.0.0 (adiciona campo confidence)
class GrammarErrorSchemaV2(BaseModel):
    type: str
    original: str
    corrected: str
    confidence: float = 0.9  # Novo campo com valor padr√£o

# Vers√£o 3.0.0 (adiciona campo position)
class GrammarErrorSchemaV3(BaseModel):
    type: str
    original: str
    corrected: str
    confidence: float = 0.9
    position: Optional[Dict[str, int]] = None  # Novo campo opcional

# Schema atual (usa vers√£o mais recente)
class GrammarErrorSchema(GrammarErrorSchemaV3):
    """Schema atual - sempre usa vers√£o mais recente"""
    pass

# Fun√ß√£o de migra√ß√£o
def migrate_grammar_error(data: Dict, from_version: str, to_version: str) -> Dict:
    """Migra dados entre vers√µes"""
    if from_version == "1.0.0" and to_version == "2.0.0":
        # Adiciona confidence com valor padr√£o
        data["confidence"] = 0.9
        return data
    elif from_version == "2.0.0" and to_version == "3.0.0":
        # Adiciona position como None
        data["position"] = None
        return data
    return data

# Valida√ß√£o com versionamento
def validate_with_version(data: Dict, version: str) -> Dict:
    """Valida dados considerando vers√£o"""
    current_version = "3.0.0"
    
    if version == current_version:
        # Usa schema atual
        return GrammarErrorSchema(**data).dict()
    else:
        # Migra para vers√£o atual
        migrated = migrate_grammar_error(data, version, current_version)
        return GrammarErrorSchema(**migrated).dict()
```

**Implementa√ß√£o no Sistema**:

```python
# Arquivo: backend/app/schemas/analysis_schemas.py

# Vers√µes suportadas
SCHEMA_VERSIONS = {
    "grammar_error": {
        "1.0.0": GrammarErrorSchemaV1,
        "2.0.0": GrammarErrorSchemaV2,
        "3.0.0": GrammarErrorSchemaV3,
        "current": "3.0.0"
    },
    "vocabulary": {
        "1.0.0": VocabularySuggestionSchemaV1,
        "2.0.0": VocabularySuggestionSchemaV2,
        "current": "2.0.0"
    }
}

def validate_analysis_data(data: Dict, schema_type: str, version: str = None) -> Dict:
    """Valida dados de an√°lise com suporte a versionamento"""
    if version is None:
        version = SCHEMA_VERSIONS[schema_type]["current"]
    
    schema_class = SCHEMA_VERSIONS[schema_type].get(version)
    if not schema_class:
        # Tenta migrar para vers√£o atual
        version = SCHEMA_VERSIONS[schema_type]["current"]
        schema_class = SCHEMA_VERSIONS[schema_type][version]
        data = migrate_data(data, schema_type, version)
    
    return schema_class(**data).dict()
```

**Migra√ß√£o de Dados Existentes**:

**Arquivo**: `backend/app/services/schema_migrator.py`

```python
class SchemaMigrator:
    """Migra dados antigos para vers√µes atuais dos schemas"""
    
    def migrate_all_messages(self, db: Session) -> Dict:
        """Migra todas as mensagens no banco"""
        messages = db.query(ChatMessage).filter(
            ChatMessage.grammar_errors.isnot(None)
        ).all()
        
        migrated = 0
        errors = 0
        
        for message in messages:
            try:
                # Verifica vers√£o atual
                metadata = message.analysis_metadata or {}
                version = metadata.get("schema_version", "1.0.0")
                
                # Migra se necess√°rio
                if version != SCHEMA_VERSIONS["grammar_error"]["current"]:
                    message.grammar_errors = self._migrate_grammar_errors(
                        message.grammar_errors,
                        version
                    )
                    metadata["schema_version"] = SCHEMA_VERSIONS["grammar_error"]["current"]
                    message.analysis_metadata = metadata
                    migrated += 1
                    
            except Exception as e:
                logger.error(f"Erro ao migrar mensagem {message.id}: {e}")
                errors += 1
        
        db.commit()
        return {
            "migrated": migrated,
            "errors": errors,
            "total": len(messages)
        }
    
    def _migrate_grammar_errors(self, errors: List[Dict], from_version: str) -> List[Dict]:
        """Migra lista de erros gramaticais"""
        current_version = SCHEMA_VERSIONS["grammar_error"]["current"]
        migrated = []
        
        for error in errors:
            migrated_error = migrate_grammar_error(error, from_version, current_version)
            migrated.append(migrated_error)
        
        return migrated
```

#### 4. Valida√ß√£o de Dados Existentes

**Tarefa de Migra√ß√£o**:
- Script para validar dados existentes
- Corrigir dados inv√°lidos automaticamente quando poss√≠vel
- Reportar dados que precisam corre√ß√£o manual
- **Migrar dados antigos para vers√µes atuais dos schemas**

**Arquivo**: `backend/app/services/data_validator.py`

```python
class DataValidator:
    def validate_all_messages(self) -> Dict:
        """Valida todas as mensagens no banco"""
        
    def fix_invalid_data(self, message_id: UUID) -> bool:
        """Tenta corrigir dados inv√°lidos"""
        
    def report_invalid_data(self) -> List[Dict]:
        """Retorna lista de dados inv√°lidos que precisam corre√ß√£o manual"""
```

**Arquivo**: `backend/app/services/schema_migrator.py`

```python
class SchemaMigrator:
    """Migra dados antigos para vers√µes atuais dos schemas"""
    
    def migrate_all_messages(self, db: Session) -> Dict:
        """Migra todas as mensagens no banco para schemas atuais"""
        messages = db.query(ChatMessage).filter(
            ChatMessage.grammar_errors.isnot(None)
        ).all()
        
        migrated = 0
        errors = 0
        
        for message in messages:
            try:
                # Verifica vers√£o atual
                metadata = message.analysis_metadata or {}
                version = metadata.get("schema_version", "1.0.0")
                current_version = SCHEMA_VERSIONS["grammar_error"]["current"]
                
                # Migra se necess√°rio
                if version != current_version:
                    message.grammar_errors = self._migrate_grammar_errors(
                        message.grammar_errors,
                        version,
                        current_version
                    )
                    metadata["schema_version"] = current_version
                    message.analysis_metadata = metadata
                    migrated += 1
                    
            except Exception as e:
                logger.error(f"Erro ao migrar mensagem {message.id}: {e}")
                errors += 1
        
        db.commit()
        return {
            "migrated": migrated,
            "errors": errors,
            "total": len(messages)
        }
    
    def _migrate_grammar_errors(
        self, 
        errors: List[Dict], 
        from_version: str,
        to_version: str
    ) -> List[Dict]:
        """Migra lista de erros gramaticais entre vers√µes"""
        migrated = []
        
        for error in errors:
            # Aplica migra√ß√µes incrementais
            current = error
            versions = ["1.0.0", "2.0.0", "3.0.0"]
            start_idx = versions.index(from_version)
            end_idx = versions.index(to_version)
            
            for i in range(start_idx, end_idx):
                current = migrate_grammar_error(
                    current, 
                    versions[i], 
                    versions[i + 1]
                )
            
            migrated.append(current)
        
        return migrated
```

**Script de Migra√ß√£o**: `backend/scripts/migrate_schemas.py`

```python
"""
Script para migrar dados existentes para vers√µes atuais dos schemas
Executar: python -m scripts.migrate_schemas
"""
from app.database import SessionLocal
from app.services.schema_migrator import SchemaMigrator

def main():
    db = SessionLocal()
    try:
        migrator = SchemaMigrator()
        result = migrator.migrate_all_messages(db)
        
        print(f"Migra√ß√£o conclu√≠da:")
        print(f"  - Total: {result['total']}")
        print(f"  - Migradas: {result['migrated']}")
        print(f"  - Erros: {result['errors']}")
    finally:
        db.close()

if __name__ == "__main__":
    main()
```

#### 5. Testes de Valida√ß√£o

**Testes Unit√°rios**:
- Testar cada schema individualmente
- Testar casos de borda (valores nulos, tipos incorretos)
- Testar valida√ß√£o de vers√µes

**Testes de Integra√ß√£o**:
- Testar fluxo completo com dados v√°lidos
- Testar rejei√ß√£o de dados inv√°lidos
- Testar migra√ß√£o de vers√µes

## Batch Processing - Explica√ß√£o Detalhada (Ponto 1 e 7)

### O que √© Batch Processing?

**Batch Processing** (Processamento em Lote) √© uma t√©cnica onde m√∫ltiplas tarefas s√£o processadas juntas em grupos (lotes), ao inv√©s de uma por vez em tempo real.

### Por que usar Batch Processing?

#### Vantagens:
1. **Efici√™ncia de Recursos**: Processa muitas mensagens de uma vez, aproveitando melhor CPU/mem√≥ria
2. **Economia de API Calls**: Agrupa chamadas de LLM, reduzindo custos
3. **N√£o Bloqueia Usu√°rio**: An√°lises pesadas n√£o afetam resposta imediata
4. **Processamento Otimizado**: Pode processar em hor√°rios de baixo tr√°fego
5. **Recupera√ß√£o de Falhas**: Se uma an√°lise falhar, outras continuam

#### Exemplo Pr√°tico:

**Sem Batch Processing**:
```
Usu√°rio 1 envia mensagem ‚Üí Analisa imediatamente (5s) ‚Üí Resposta
Usu√°rio 2 envia mensagem ‚Üí Analisa imediatamente (5s) ‚Üí Resposta
Usu√°rio 3 envia mensagem ‚Üí Analisa imediatamente (5s) ‚Üí Resposta
Total: 15 segundos de processamento
```

**Com Batch Processing**:
```
Usu√°rio 1 envia mensagem ‚Üí Resposta imediata (0.5s)
Usu√°rio 2 envia mensagem ‚Üí Resposta imediata (0.5s)
Usu√°rio 3 envia mensagem ‚Üí Resposta imediata (0.5s)
[Background] Processa 3 mensagens juntas (8s) ‚Üí Atualiza banco
Total: 1.5s para usu√°rios + 8s em background
```

### Como Funciona no Nosso Sistema?

#### Fluxo de Batch Processing:

```
1. Usu√°rios enviam mensagens durante o dia
   ‚Üì
2. Mensagens s√£o armazenadas sem an√°lise completa
   ‚Üì
3. Quando backend inicia, Batch Processor executa automaticamente:
   ‚Üì
4. Agrupa mensagens n√£o analisadas (ex: 100 por vez)
   ‚Üì
5. Processa em lote:
   - Analisa erros gramaticais
   - Extrai vocabul√°rio
   - Calcula dificuldade
   - Identifica t√≥picos
   ‚Üì
6. Atualiza banco de dados com an√°lises
   ‚Üì
7. Atualiza contexto de aprendizado do usu√°rio
   ‚Üì
8. Gera embeddings e armazena no banco vetorial
```

#### Tipos de Processamento:

**1. Processamento Imediato (S√≠ncrono)**:
- An√°lise b√°sica (dificuldade simples)
- Resposta do LLM
- Armazenamento da mensagem

**2. Processamento em Lote (Ass√≠ncrono)**:
- An√°lise detalhada
- Gera√ß√£o de embeddings
- Atualiza√ß√£o de contexto
- C√°lculo de m√©tricas agregadas

### Configura√ß√µes Recomendadas

Com base no sistema atual (chaves free com boa disponibilidade):

#### Configura√ß√£o Padr√£o Sugerida:

```python
BATCH_PROCESSING_CONFIG = {
    # Executa no startup do backend (n√£o scheduler peri√≥dico)
    "trigger": "startup",
    "execution_time": "02:00",
    
    # Tamanho de lote: 100 mensagens por vez
    # (balanceia velocidade vs. uso de mem√≥ria)
    "batch_size": 100,
    
    # Processar at√© 10 usu√°rios simultaneamente
    "max_concurrent_users": 10,
    
    # Priorizar usu√°rios ativos recentemente
    "priority_strategy": "active_recent",
    
    # Retry: 3 tentativas com 5 minutos entre elas
    "retry_attempts": 3,
    "retry_delay_seconds": 300,
    
    # Notificar em caso de falha persistente
    "notification_on_failure": True,
    
    # Coletar m√©tricas de performance
    "metrics_collection": True
}
```

#### Estrat√©gias de Prioriza√ß√£o:

**1. Active Recent (Recomendado)**:
- Usu√°rios que enviaram mensagens nas √∫ltimas 24h
- Garante an√°lise r√°pida para usu√°rios ativos

**2. Most Messages**:
- Usu√°rios com mais mensagens n√£o analisadas
- Reduz backlog rapidamente

**3. Round Robin**:
- Processa todos os usu√°rios igualmente
- Justo, mas pode ser mais lento

**4. Premium First**:
- Usu√°rios premium primeiro (se houver)
- Depois usu√°rios normais

### Tratamento de Falhas

#### Estrat√©gia de Retry:

```
Tentativa 1: Processa lote
  ‚Üì (falha)
Aguarda 5 minutos
  ‚Üì
Tentativa 2: Processa lote novamente
  ‚Üì (falha)
Aguarda 5 minutos
  ‚Üì
Tentativa 3: Processa lote novamente
  ‚Üì (falha)
Marca para revis√£o manual
  ‚Üì
Notifica administrador (opcional)
```

#### Tipos de Falhas:

1. **Falha Tempor√°ria** (rate limit, timeout):
   - Retry autom√°tico
   - Geralmente resolve na pr√≥xima tentativa

2. **Falha de Dados** (dados inv√°lidos):
   - Marca mensagem como "an√°lise falhou"
   - Loga erro para corre√ß√£o manual
   - Continua com pr√≥ximas mensagens

3. **Falha de Sistema** (banco offline, API down):
   - Retry com backoff exponencial
   - Notifica administrador
   - Pausa processamento at√© resolu√ß√£o

## Batch Processing - Especifica√ß√µes (Ponto 7)

### Perguntas para Definir Configura√ß√µes

#### 1. Frequ√™ncia de Execu√ß√£o
- **Pergunta**: Com que frequ√™ncia o batch processing deve ser executado?
  - **Resposta Definida**: No startup do backend (quando backend reinicia)
  - **Raz√£o**: Sistema n√£o fica ligado full time, processa quando backend inicia
  - **Implementa√ß√£o**: Evento `@app.on_event("startup")` no FastAPI

#### 2. Tamanho de Lote
- **Pergunta**: Quantas mensagens processar por vez?
  - **Resposta Definida**: 100 mensagens do usu√°rio por lote
  - **Esclarecimento**: 100 mensagens = 100 mensagens com `role="user"` (n√£o inclui respostas do assistant)
  - **Considera√ß√µes**: Mem√≥ria dispon√≠vel, tempo de processamento, limites de API

#### 3. Estrat√©gia de Prioriza√ß√£o
- **Pergunta**: Como priorizar usu√°rios para processamento?
  - Op√ß√µes:
    - Usu√°rios mais ativos primeiro
    - Usu√°rios com mais mensagens n√£o analisadas
    - Round-robin
    - Baseado em √∫ltima an√°lise
  - **Recomenda√ß√£o**: Usu√°rios ativos recentemente + mensagens n√£o analisadas

#### 4. Tratamento de Falhas
- **Pergunta**: Como lidar com falhas em lote?
  - Op√ß√µes:
    - Retry autom√°tico (quantas tentativas?)
    - Marcar para processamento posterior
    - Notificar administrador
    - Continuar com pr√≥ximo usu√°rio
  - **Recomenda√ß√£o**: 3 tentativas, depois marcar para revis√£o manual

#### 5. Processamento em Tempo Real vs. Batch
- **Pergunta**: Algumas an√°lises devem ser processadas imediatamente?
  - Op√ß√µes:
    - Todas ass√≠ncronas
    - An√°lise b√°sica s√≠ncrona, detalhada ass√≠ncrona
    - Prioridade alta para usu√°rios premium
  - **Recomenda√ß√£o**: An√°lise b√°sica pode ser s√≠ncrona, detalhada ass√≠ncrona

#### 6. Recursos e Limites
- **Pergunta**: Quais s√£o os limites de recursos dispon√≠veis?
  - Considera√ß√µes: CPU, mem√≥ria, cotas de API LLM
  - **Recomenda√ß√£o**: Processar em hor√°rios de baixo tr√°fego

#### 7. M√©tricas e Monitoramento
- **Pergunta**: Quais m√©tricas devem ser coletadas?
  - Op√ß√µes:
    - Tempo de processamento
    - Taxa de sucesso/falha
    - Mensagens processadas por hora
    - Uso de recursos
  - **Recomenda√ß√£o**: Todas as op√ß√µes acima

### Configura√ß√£o Padr√£o (Definida)

Com base na an√°lise do sistema e disponibilidade de recursos:

```python
BATCH_PROCESSING_CONFIG = {
    "trigger": "startup",  # Executa quando backend inicia (n√£o scheduler peri√≥dico)
    "batch_size": 100,  # 100 mensagens do usu√°rio (role="user") por lote
    "max_concurrent_users": 10,  # Processar 10 usu√°rios simultaneamente
    "priority_strategy": "active_recent",  # Usu√°rios ativos recentemente
    "retry_attempts": 3,
    "retry_delay_seconds": 300,  # 5 minutos entre tentativas
    "notification_on_failure": True,
    "metrics_collection": True,
    "process_only_pending": True  # Processa apenas mensagens n√£o analisadas
}
```

**Justificativa**:
- **Trigger no startup**: Sistema n√£o fica ligado full time, processa quando backend reinicia
- **100 mensagens do usu√°rio**: Refere-se a mensagens com `role="user"` (n√£o inclui respostas do assistant)
- **Processamento local**: Adequado para ambiente de desenvolvimento/testes
- **10 usu√°rios simult√¢neos**: Aproveita recursos sem sobrecarregar
- **Active recent**: Prioriza usu√°rios que mais precisam de an√°lise atualizada

**Nota sobre Mensagens**:
- **100 mensagens** = 100 mensagens enviadas pelo usu√°rio no chat (`ChatMessage.role == "user"`)
- **N√ÉO** inclui mensagens do assistant (`role == "assistant"`)
- Cada mensagem do usu√°rio √© uma intera√ß√£o que precisa ser analisada

## Isolamento de Dados por Usu√°rio (Ponto 9)

### Objetivo
Garantir que dados de diferentes usu√°rios sejam completamente isolados, sem necessidade de criptografia, apenas isolamento l√≥gico.

### Estrat√©gias de Isolamento

#### 1. Banco Vetorial - Filtros Obrigat√≥rios

**Todas as opera√ß√µes devem incluir filtro `user_id`**:

```python
class VectorStore:
    def search_similar(
        self,
        collection: str,
        query_embedding: List[float],
        user_id: UUID,  # OBRIGAT√ìRIO
        limit: int = 10,
        additional_filters: Optional[Dict] = None
    ) -> List[Dict]:
        """Busca sempre filtrada por user_id"""
        filters = {"user_id": str(user_id)}
        if additional_filters:
            filters.update(additional_filters)
        # ...
```

#### 2. Collections Particionadas por Usu√°rio

**Op√ß√£o A: Namespace por Usu√°rio**
- Cada usu√°rio tem seu pr√≥prio namespace: `user_messages_{user_id}`
- Isolamento completo, mas mais collections

**Op√ß√£o B: Filtro Obrigat√≥rio (Recomendado)**
- Uma collection compartilhada com filtro `user_id` obrigat√≥rio
- Mais eficiente, mas requer valida√ß√£o rigorosa

**Recomenda√ß√£o**: Op√ß√£o B com valida√ß√£o obrigat√≥ria de `user_id` em todas as opera√ß√µes.

#### 3. Valida√ß√£o de Acesso

**Middleware de Valida√ß√£o**:
```python
def validate_user_access(user_id: UUID, requested_user_id: UUID):
    """Valida que usu√°rio s√≥ acessa seus pr√≥prios dados"""
    if user_id != requested_user_id:
        raise PermissionError("Access denied")
```

#### 4. Backup e Limpeza por Usu√°rio

**Opera√ß√µes Isoladas**:
- Backup pode ser feito por usu√°rio espec√≠fico
- Limpeza pode ser feita por usu√°rio espec√≠fico
- Exporta√ß√£o de dados por usu√°rio

#### 5. Logs e Auditoria

**Rastreamento**:
- Logs devem incluir `user_id` em todas as opera√ß√µes
- Auditoria de acessos a dados
- Detec√ß√£o de tentativas de acesso n√£o autorizado

## Testes e Qualidade (Ponto 10 - Detalhado)

### Objetivo
Garantir qualidade e confiabilidade do sistema atrav√©s de testes abrangentes em todas as camadas.

### Estrat√©gia de Testes

#### 1. Testes Unit√°rios

**Cobertura**: Cada fun√ß√£o/m√©todo isoladamente

**Arquivos de Teste**:
- `backend/tests/unit/services/test_message_analyzer.py`
- `backend/tests/unit/services/test_language_normalizer.py`
- `backend/tests/unit/services/test_session_context_manager.py`
- `backend/tests/unit/modules/vector_db/test_embedding_service.py`
- `backend/tests/unit/modules/vector_db/test_semantic_search.py`

**Exemplos**:
```python
def test_message_analyzer_extract_grammar_errors():
    analyzer = MessageAnalyzer()
    message = "I goes to school"  # J√° em ingl√™s
    errors = analyzer._extract_grammar_errors(message, "en")
    assert len(errors) > 0
    assert errors[0]["type"] == "verb_tense"
    assert errors[0]["original"] == "goes"
    assert errors[0]["corrected"] == "go"

def test_language_normalizer_translate_to_english():
    normalizer = LanguageNormalizer()
    text = "Ol√°, como voc√™ est√°?"
    normalized = normalizer.normalize_for_storage(text, "pt")
    assert normalized == "Hello, how are you?"
```

#### 2. Testes de Integra√ß√£o

**Cobertura**: Fluxo completo entre componentes

**Arquivos de Teste**:
- `backend/tests/integration/test_message_analysis_flow.py`
- `backend/tests/integration/test_vector_db_integration.py`
- `backend/tests/integration/test_rag_flow.py`

**Exemplos**:
```python
def test_complete_message_analysis_flow():
    # 1. Criar mensagem
    # 2. Normalizar para ingl√™s
    # 3. Analisar
    # 4. Armazenar no banco
    # 5. Verificar dados armazenados
    pass

def test_vector_db_storage_and_retrieval():
    # 1. Armazenar embedding
    # 2. Buscar similar
    # 3. Verificar resultados
    pass
```

#### 3. Testes de Performance

**Cobertura**: Tempo de resposta, uso de recursos, escalabilidade

**Arquivos de Teste**:
- `backend/tests/performance/test_embedding_generation.py`
- `backend/tests/performance/test_semantic_search.py`
- `backend/tests/performance/test_batch_processing.py`

**M√©tricas a Testar**:
- Tempo de gera√ß√£o de embedding (deve ser < 100ms)
- Tempo de busca sem√¢ntica (deve ser < 500ms)
- Throughput de processamento em lote
- Uso de mem√≥ria durante processamento

**Exemplos**:
```python
def test_embedding_generation_performance():
    service = EmbeddingService()
    text = "Test message"
    
    start = time.time()
    embedding = service.generate_embedding(text)
    duration = time.time() - start
    
    assert duration < 0.1  # Menos de 100ms
    assert len(embedding) > 0

def test_semantic_search_performance():
    # Testar busca com 1000 embeddings
    # Verificar tempo de resposta
    pass
```

#### 4. Testes de Carga

**Cobertura**: Sistema sob carga alta

**Cen√°rios**:
- M√∫ltiplos usu√°rios enviando mensagens simultaneamente
- Processamento em lote de muitos usu√°rios
- Busca sem√¢ntica com muitos embeddings

**Ferramentas**: `locust`, `pytest-benchmark`

#### 5. Testes de Valida√ß√£o de Dados

**Cobertura**: Schemas Pydantic e valida√ß√£o de JSONB

**Arquivos de Teste**:
- `backend/tests/validation/test_analysis_schemas.py`
- `backend/tests/validation/test_data_integrity.py`

**Exemplos**:
```python
def test_grammar_error_schema_validation():
    valid_data = {
        "type": "verb_tense",
        "original": "goes",
        "corrected": "go",
        "confidence": 0.95
    }
    error = GrammarErrorSchema(**valid_data)
    assert error.type == "verb_tense"

def test_invalid_confidence_score():
    invalid_data = {
        "type": "verb_tense",
        "original": "goes",
        "corrected": "go",
        "confidence": 1.5  # Inv√°lido (> 1.0)
    }
    with pytest.raises(ValidationError):
        GrammarErrorSchema(**invalid_data)
```

#### 6. Testes de Isolamento de Dados

**Cobertura**: Garantir que dados de usu√°rios n√£o se misturem

**Arquivos de Teste**:
- `backend/tests/security/test_user_isolation.py`

**Exemplos**:
```python
def test_vector_db_user_isolation():
    user1_id = UUID("...")
    user2_id = UUID("...")
    
    # Armazenar embedding para user1
    store.store_embedding(..., user_id=user1_id, ...)
    
    # Buscar para user2 (n√£o deve retornar dados de user1)
    results = store.search_similar(..., user_id=user2_id, ...)
    assert len(results) == 0
```

#### 7. Testes de Tradu√ß√£o e Normaliza√ß√£o

**Cobertura**: Tradu√ß√£o correta para ingl√™s e de volta

**Arquivos de Teste**:
- `backend/tests/unit/services/test_language_normalizer.py`

**Exemplos**:
```python
def test_normalize_and_denormalize():
    normalizer = LanguageNormalizer()
    original = "Ol√°, como voc√™ est√°?"
    
    # Normalizar para ingl√™s
    normalized = normalizer.normalize_for_storage(original, "pt")
    assert normalized == "Hello, how are you?"
    
    # Normalizar de volta para portugu√™s
    denormalized = normalizer.normalize_for_display(normalized, "pt")
    assert "ol√°" in denormalized.lower() or "como" in denormalized.lower()
```

#### 8. Estrutura de Testes

```
backend/tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îî‚îÄ‚îÄ schemas/
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_message_analysis_flow.py
‚îÇ   ‚îú‚îÄ‚îÄ test_vector_db_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_rag_flow.py
‚îú‚îÄ‚îÄ performance/
‚îÇ   ‚îú‚îÄ‚îÄ test_embedding_generation.py
‚îÇ   ‚îî‚îÄ‚îÄ test_semantic_search.py
‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îú‚îÄ‚îÄ test_analysis_schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ test_data_integrity.py
‚îî‚îÄ‚îÄ security/
    ‚îî‚îÄ‚îÄ test_user_isolation.py
```

#### 9. Cobertura de C√≥digo

**Meta**: **M√≠nimo 80% de cobertura de c√≥digo**

**Ferramenta**: `pytest-cov`

**Comando**:
```bash
# Executar testes com cobertura
pytest --cov=app --cov-report=html --cov-report=term

# Verificar se cobertura est√° acima de 80%
pytest --cov=app --cov-report=term --cov-fail-under=80
```

**Configura√ß√£o no `pytest.ini` ou `pyproject.toml`**:
```ini
[tool.pytest.ini_options]
addopts = --cov=app --cov-report=html --cov-report=term --cov-fail-under=80
```

**Aviso de Erro**:
- Se cobertura < 80%, testes falham
- CI/CD bloqueia merge se cobertura insuficiente
- Relat√≥rio HTML mostra linhas n√£o cobertas

#### 10. Estrutura de Execu√ß√£o de Testes

**Execu√ß√£o Separada (Desenvolvimento)**:
```bash
# Testes unit√°rios (r√°pidos, executar frequentemente)
pytest tests/unit/ -v

# Testes de integra√ß√£o (mais lentos)
pytest tests/integration/ -v

# Testes de performance (executar manualmente)
pytest tests/performance/ -v --benchmark-only

# Todos os testes
pytest tests/ -v
```

**CI/CD (Pipeline)**:
```yaml
# .github/workflows/tests.yml ou similar
steps:
  - name: Run Unit Tests
    run: pytest tests/unit/ --cov=app --cov-fail-under=80
    
  - name: Run Integration Tests
    run: pytest tests/integration/
    
  - name: Upload Coverage Report
    uses: codecov/codecov-action@v3
```

**Estrat√©gia de Execu√ß√£o**:
1. **Durante Desenvolvimento**: Executar testes unit√°rios localmente
2. **Antes de Commit**: Executar todos os testes localmente
3. **No CI/CD**: Executar todos os testes automaticamente
4. **Testes de Performance**: Executar separadamente, n√£o bloqueia CI

#### 11. Testes Cont√≠nuos

**CI/CD**:
- Executar testes unit√°rios em cada commit
- Executar testes de integra√ß√£o em PRs
- Executar testes de performance separadamente (n√£o bloqueia)
- Falhar build se cobertura < 80%

## Otimiza√ß√µes de Armazenamento

### Estrat√©gias

#### PostgreSQL

1. **√çndices**:
   - `chat_messages.created_at` (j√° existe)
   - `chat_messages.session_id` (j√° existe)
   - `chat_sessions.user_id` (j√° existe)
   - Adicionar √≠ndice em `chat_messages.role` para filtrar mensagens do usu√°rio
   - Adicionar √≠ndice GIN em campos JSONB (`grammar_errors`, `topics`, `vocabulary_suggestions`)

2. **Particionamento** (futuro):
   - Particionar `chat_messages` por data (mensal)
   - Particionar `token_usage` por data

3. **Compress√£o**:
   - Comprimir mensagens antigas (> 6 meses)
   - Manter apenas an√°lises agregadas para mensagens muito antigas

4. **Limpeza**:
   - Manter an√°lises detalhadas por 90 dias
   - Ap√≥s 90 dias, manter apenas m√©tricas agregadas

#### Banco Vetorial

1. **√çndices Vetoriais**:
   - Usar HNSW (Hierarchical Navigable Small World) para busca r√°pida
   - Configurar par√¢metros de √≠ndice (M, ef_construction) para balancear velocidade/precis√£o

2. **Particionamento por Usu√°rio**:
   - **Filtro obrigat√≥rio `user_id`** em todas as opera√ß√µes
   - √çndice em `user_id` para busca eficiente
   - Facilitar backup e limpeza por usu√°rio
   - Valida√ß√£o de acesso em todas as opera√ß√µes

3. **Cache de Embeddings**:
   - Cachear embeddings de mensagens frequentes
   - Reduzir rec√°lculo de embeddings

4. **Limpeza Autom√°tica**:
   - Remover embeddings de mensagens deletadas
   - Limpar embeddings antigos (> 1 ano) periodicamente
   - Manter apenas embeddings mais relevantes

5. **Otimiza√ß√£o de Busca**:
   - Limitar busca a usu√°rio espec√≠fico
   - Usar filtros de metadata para reduzir espa√ßo de busca
   - Implementar busca hier√°rquica (primeiro por usu√°rio, depois sem√¢ntica)

## Estrutura de Arquivos

```
backend/app/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ language_normalizer.py       # Fase 1 - Normaliza√ß√£o de idioma
‚îÇ   ‚îú‚îÄ‚îÄ message_analyzer.py          # Fase 1
‚îÇ   ‚îú‚îÄ‚îÄ session_context_manager.py    # Fase 2
‚îÇ   ‚îú‚îÄ‚îÄ progress_tracker.py           # Fase 3
‚îÇ   ‚îú‚îÄ‚îÄ batch_analyzer.py             # Fase 3
‚îÇ   ‚îú‚îÄ‚îÄ context_enricher.py           # Fase 4
‚îÇ   ‚îî‚îÄ‚îÄ (scheduler removido - usa startup event)
‚îÇ   ‚îî‚îÄ‚îÄ data_validator.py             # Valida√ß√£o de dados
‚îÇ
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py                    # Schemas principais
‚îÇ   ‚îî‚îÄ‚îÄ analysis_schemas.py           # Schemas de valida√ß√£o (Fase 1.5)
‚îÇ
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îî‚îÄ‚îÄ vector_db/                    # M√≥dulo de Banco Vetorial (Fases 5-7)
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ vector_store.py           # Interface e implementa√ß√£o base
‚îÇ       ‚îú‚îÄ‚îÄ embedding_service.py     # Gera√ß√£o de embeddings
‚îÇ       ‚îú‚îÄ‚îÄ collections.py            # Defini√ß√£o de collections
‚îÇ       ‚îú‚îÄ‚îÄ config.py                # Configura√ß√µes
‚îÇ       ‚îú‚îÄ‚îÄ chroma_store.py           # Implementa√ß√£o ChromaDB (ou alternativa)
‚îÇ       ‚îú‚îÄ‚îÄ semantic_search.py       # Busca sem√¢ntica (Fase 6)
‚îÇ       ‚îú‚îÄ‚îÄ rag_service.py           # RAG Service (Fase 6)
‚îÇ       ‚îú‚îÄ‚îÄ batch_embedding_processor.py  # Processamento em lote (Fase 7)
‚îÇ       ‚îî‚îÄ‚îÄ manager.py               # Gerenciamento geral (Fase 7)
‚îÇ
‚îî‚îÄ‚îÄ migrations/
    ‚îî‚îÄ‚îÄ add_topics_to_chat_message.py # Fase 1
```

## Depend√™ncias

### Novas Depend√™ncias

#### Fases 1-4
- **Pydantic**: `pydantic>=2.0.0` (para valida√ß√£o de schemas)
- N√£o necess√°rio scheduler peri√≥dico (batch executa no startup)
- Opcional: `redis` (para cache de tradu√ß√µes e filas)

#### Fases 5-7 (Banco Vetorial)
- **ChromaDB**: `chromadb>=0.4.0` (ou alternativa: Pinecone, pgvector)
- **Sentence Transformers**: `sentence-transformers>=2.2.0` (para embeddings)
- **NumPy**: `numpy>=1.24.0` (para opera√ß√µes vetoriais)
- **Opcional**: `faiss-cpu` ou `faiss-gpu` (para busca vetorial otimizada)
- **Opcional**: `openai` (se usar OpenAI embeddings)

## M√©tricas de Sucesso

1. **Fase 1**: 
   - Mensagens analisadas com erros, vocabul√°rio e dificuldade (em ingl√™s)
   - An√°lise ass√≠ncrona n√£o impacta lat√™ncia (< 2s para resposta)
   - Taxa de sucesso de an√°lise > 95%
   - Tempo m√©dio de an√°lise < 5s (ass√≠ncrono)

2. **Fase 2**: 
   - `session_context` atualizado dinamicamente (em ingl√™s)
   - Atualiza√ß√£o em tempo real durante sess√£o

3. **Fase 3**: 
   - `learning_context` reflete progresso real do usu√°rio (em ingl√™s)
   - Batch processing executa dentro do tempo configurado
   - Taxa de sucesso de processamento em lote > 90%

4. **Fase 4**: 
   - Prompts personalizados melhoram qualidade das respostas
   - Contexto relevante recuperado com precis√£o > 80%

5. **Fase 5**: 
   - Banco vetorial operacional com armazenamento de embeddings
   - Isolamento por usu√°rio garantido (100% das opera√ß√µes filtradas)
   - Tempo de armazenamento < 200ms

6. **Fase 6**: 
   - Busca sem√¢ntica recupera contexto relevante com precis√£o > 80%
   - Tempo de busca < 500ms

7. **Fase 7**: 
   - Sistema completo integrado com RAG melhorando qualidade das respostas
   - Cobertura de testes > 80%
   - Zero vazamento de dados entre usu√°rios

## Fluxo Completo com Banco Vetorial

### Fluxo de Armazenamento
1. Usu√°rio envia mensagem
2. Mensagem √© analisada (Fase 1)
3. Embedding √© gerado da mensagem
4. Embedding √© armazenado no banco vetorial com metadados
5. T√≥picos, erros e vocabul√°rio tamb√©m s√£o armazenados como embeddings

### Fluxo de Recupera√ß√£o (RAG)
1. Nova mensagem do usu√°rio
2. Embedding da mensagem √© gerado
3. Busca sem√¢ntica no banco vetorial:
   - Mensagens similares
   - T√≥picos relacionados
   - Erros e corre√ß√µes similares
   - Vocabul√°rio relacionado
4. Contexto recuperado √© formatado
5. Contexto √© inclu√≠do no prompt do LLM
6. Resposta √© gerada com contexto enriquecido

## Implementa√ß√£o de Tradu√ß√£o para Ingl√™s - Detalhamento

### Vis√£o Geral

Todas as informa√ß√µes coletadas, analisadas e armazenadas devem estar em **ingl√™s** para:
- **Padroniza√ß√£o**: Consist√™ncia entre diferentes idiomas de origem
- **Performance**: Modelos de embedding e LLMs performam melhor em ingl√™s
- **Busca Sem√¢ntica**: Embeddings em ingl√™s s√£o mais precisos
- **Manuten√ß√£o**: C√≥digo e l√≥gica mais simples com um √∫nico idioma de armazenamento

### Pontos de Tradu√ß√£o

#### 1. Entrada de Dados (Normaliza√ß√£o)

**Quando**: Antes de qualquer an√°lise ou armazenamento

**O que traduzir**:
- Texto da mensagem do usu√°rio
- T√≥picos identificados
- Tipos de erros gramaticais
- Vocabul√°rio extra√≠do
- Contexto de sess√£o
- Insights e m√©tricas

**Exemplo**:
```python
# Usu√°rio escreve em portugu√™s
user_message = "Eu gosto de comer pizza"

# Normalizar para ingl√™s antes de analisar
normalized_message = normalizer.normalize_for_storage(
    user_message, 
    source_language="pt"
)
# normalized_message = "I like to eat pizza"

# An√°lise √© feita em ingl√™s
analysis = analyzer.analyze_message(normalized_message, ...)
# analysis["topics"] = ["food", "preferences"]  # Em ingl√™s
```

#### 2. Armazenamento

**PostgreSQL (Campos JSONB)**:
- `grammar_errors`: Erros em ingl√™s
- `vocabulary_suggestions`: Vocabul√°rio em ingl√™s
- `topics`: T√≥picos em ingl√™s
- `session_context`: Todo contexto em ingl√™s
- `learning_context`: Todo contexto em ingl√™s

**Banco Vetorial**:
- Embeddings gerados de textos em ingl√™s
- Metadata em ingl√™s (t√≥picos, tipos de erro, etc.)

#### 3. Recupera√ß√£o e Exibi√ß√£o (Denormaliza√ß√£o)

**Quando**: Ao retornar dados para o usu√°rio

**O que traduzir de volta**:
- Mensagens e contexto para exibi√ß√£o
- Erros e corre√ß√µes
- T√≥picos
- Vocabul√°rio
- Insights e recomenda√ß√µes

**Exemplo**:
```python
# Dados armazenados em ingl√™s
stored_topic = "food"
stored_error = {
    "type": "verb_tense",
    "original": "goes",
    "corrected": "go"
}

# Traduzir de volta para portugu√™s para exibi√ß√£o
display_topic = normalizer.normalize_for_display(
    stored_topic,
    target_language="pt"
)
# display_topic = "comida"

display_error = {
    "type": normalizer.normalize_for_display("verb_tense", "pt"),  # "tempo verbal"
    "original": normalizer.normalize_for_display("goes", "pt"),    # "vai"
    "corrected": normalizer.normalize_for_display("go", "pt")      # "vai"
}
```

### Estrat√©gia de Implementa√ß√£o

#### 1. Dicion√°rio de Termos T√©cnicos

**Arquivo**: `backend/app/services/language_normalizer.py`

Criar dicion√°rio de termos que n√£o precisam tradu√ß√£o (ou t√™m tradu√ß√£o fixa):
```python
TECHNICAL_TERMS = {
    "verb_tense": {
        "pt": "tempo verbal",
        "es": "tiempo verbal",
        "fr": "temps verbal"
    },
    "article": {
        "pt": "artigo",
        "es": "art√≠culo",
        "fr": "article"
    },
    # ... mais termos
}
```

#### 2. Cache de Tradu√ß√µes

**Estrat√©gia**:
- Cachear tradu√ß√µes frequentes (ex: "verb_tense" sempre = "tempo verbal")
- Usar Redis ou cache em mem√≥ria
- TTL de 30 dias para tradu√ß√µes

**Exemplo**:
```python
def _get_cached_translation(self, text: str, source: str, target: str) -> Optional[str]:
    cache_key = f"translation:{source}:{target}:{hash(text)}"
    return self.cache.get(cache_key)

def _cache_translation(self, text: str, source: str, target: str, translated: str):
    cache_key = f"translation:{source}:{target}:{hash(text)}"
    self.cache.set(cache_key, translated, ttl=2592000)  # 30 dias
```

#### 3. Uso de LLM para Tradu√ß√£o

**Reutilizar infraestrutura existente**:
- Usar mesmo LLM service j√° configurado
- Usar chaves de API free dispon√≠veis
- Batch de tradu√ß√µes para efici√™ncia

**Prompt de Tradu√ß√£o**:
```python
def _translate_with_llm(self, text: str, source: str, target: str) -> str:
    prompt = f"""Translate the following text from {source} to {target}.
Maintain the meaning and context exactly.
Only return the translation, nothing else.

Text: {text}
Translation:"""
    
    # Usar LLM service existente
    translation = self.llm_service.generate_text(prompt, max_tokens=500)
    return translation.strip()
```

#### 4. Fallback para Tradu√ß√£o Manual

**Quando LLM falha**:
- Manter dicion√°rio de fallback
- Logar falhas para revis√£o
- Retornar texto original se tradu√ß√£o falhar (com flag de aviso)

### Fluxo Completo de Normaliza√ß√£o

```
1. Usu√°rio envia: "Eu gosto de pizza"
   ‚Üì
2. LanguageNormalizer.normalize_for_storage()
   ‚Üí Traduz para: "I like pizza"
   ‚Üí Cacheia tradu√ß√£o
   ‚Üì
3. MessageAnalyzer.analyze()
   ‚Üí Analisa texto em ingl√™s
   ‚Üí Retorna: {"topics": ["food", "preferences"], ...}
   ‚Üì
4. Armazenamento
   ‚Üí PostgreSQL: topics = ["food", "preferences"]
   ‚Üí VectorDB: embedding de "I like pizza"
   ‚Üì
5. Recupera√ß√£o para exibi√ß√£o
   ‚Üí LanguageNormalizer.normalize_for_display()
   ‚Üí Traduz de volta: topics = ["comida", "prefer√™ncias"]
   ‚Üí Retorna para usu√°rio em portugu√™s
```

### Considera√ß√µes de Performance

1. **Cache Agressivo**: Cachear todas as tradu√ß√µes poss√≠veis
2. **Batch Translation**: Traduzir m√∫ltiplos textos de uma vez
3. **Lazy Translation**: Traduzir apenas quando necess√°rio para exibi√ß√£o
4. **Pre-translation**: Traduzir termos comuns antecipadamente

### Tratamento de Erros

1. **Falha de Tradu√ß√£o**: 
   - Logar erro
   - Armazenar texto original com flag `translation_failed`
   - Tentar novamente em processamento ass√≠ncrono

2. **Tradu√ß√£o Incorreta**:
   - Permitir corre√ß√£o manual
   - Aprender com corre√ß√µes (futuro)

3. **Idioma N√£o Suportado**:
   - Fallback para ingl√™s
   - Notificar usu√°rio

## Avalia√ß√£o da Ordem L√≥gica das Fases

### ‚úÖ Ordem L√≥gica Correta

A ordem das fases est√° **correta e bem estruturada**:

1. **Fase 1 (An√°lise B√°sica)** ‚Üí ‚úÖ **Conclu√≠da**
   - Base fundamental: coleta dados de cada mensagem
   - Independente: pode ser implementada sozinha
   - **Status**: Funcionando e testado

2. **Fase 2 (Contexto de Sess√£o)** ‚Üí ‚è∏Ô∏è **Pausada**
   - Depende da Fase 1: usa dados de an√°lise de mensagens
   - Agrega dados dentro de uma sess√£o
   - **Ordem correta**: Deve vir ap√≥s Fase 1

3. **Fase 3 (Tracking de Progresso)** ‚Üí ‚è∏Ô∏è **Pendente**
   - Depende da Fase 2: usa contexto de sess√£o agregado
   - Analisa evolu√ß√£o ao longo do tempo
   - **Ordem correta**: Deve vir ap√≥s Fase 2

4. **Fase 4 (Personaliza√ß√£o)** ‚Üí ‚è∏Ô∏è **Pendente**
   - Depende da Fase 3: usa dados de progresso
   - Aplica personaliza√ß√£o baseada em hist√≥rico
   - **Ordem correta**: Deve vir ap√≥s Fase 3

5. **Fases 5-7 (Banco Vetorial)** ‚Üí ‚è∏Ô∏è **Pendentes**
   - Dependem das Fases 1-4: usam dados coletados
   - Adicionam busca sem√¢ntica e RAG
   - **Ordem correta**: Devem vir ap√≥s Fase 2 (podem ser paralelas)

### Depend√™ncias Identificadas

```
Fase 1 (An√°lise) 
    ‚Üì
Fase 2 (Contexto Sess√£o) ‚îÄ‚îÄ‚îê
    ‚Üì                       ‚îÇ
Fase 3 (Progresso)          ‚îÇ
    ‚Üì                       ‚îÇ
Fase 4 (Personaliza√ß√£o)     ‚îÇ
                            ‚îÇ
Fases 5-7 (Vector DB) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Conclus√£o

‚úÖ **A ordem est√° correta e l√≥gica**
- Cada fase depende das anteriores
- N√£o h√° depend√™ncias circulares
- Fases podem ser implementadas sequencialmente
- Fases 5-7 podem ser paralelas ap√≥s Fase 2

### Recomenda√ß√µes

1. **Manter ordem atual**: N√£o alterar a sequ√™ncia das fases
2. **Fase 2 pode ser retomada**: Ap√≥s melhorias no chat, retomar na ordem correta
3. **Fases 5-7 podem ser planejadas**: Enquanto Fase 2 est√° pausada, pode-se planejar infraestrutura de banco vetorial

## Pr√≥ximos Passos Ap√≥s Implementa√ß√£o Completa

1. Adicionar visualiza√ß√µes de progresso no frontend
2. Criar relat√≥rios de progresso para usu√°rios
3. Implementar recomenda√ß√µes inteligentes de estudo baseadas em busca sem√¢ntica
4. Adicionar an√°lise de sentimentos e engajamento
5. Implementar sistema de gamifica√ß√£o baseado em progresso
6. Criar dashboard de analytics para professores/administradores
